{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Talk Data - Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives an introduction to working with the various data sets in [Wikipedia\n",
    "Talk](https://figshare.com/projects/Wikipedia_Talk/16731) project on Figshare. The release includes:\n",
    "\n",
    "1. a large historical corpus of discussion comments on Wikipedia talk pages\n",
    "2. a sample of over 100k comments with human labels for whether the comment contains a personal attack\n",
    "3. a sample of over 100k comments with human labels for whether the comment has aggressive tone\n",
    "\n",
    "Please refer to our [wiki](https://meta.wikimedia.org/wiki/Research:Detox/Data_Release) for documentation of the schema of each data set and our [research paper](https://arxiv.org/abs/1610.08914) for documentation on the data collection and modeling methodology. \n",
    "\n",
    "In this notebook we show how to build a simple classifier for detecting personal attacks and apply the classifier to a random sample of the comment corpus to see whether discussions on user pages have more personal attacks than discussion on article pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q & A\n",
    "\n",
    "The following code runs Python 3 and integrates Imblearn.\n",
    "\n",
    "**What text cleaning methods did you try? Which are the ones you included in the final code?**\n",
    "\n",
    "All the methods that I have tried were included in the code. I removed punctuation and extra spaces. In the vectorizer, I removed stop words and lowercased all tokens. I attempted a spellchecker, but that included the use of an API and more time. Unfortantely, I was not able to get that working.\n",
    "\n",
    "**What are the features you considered using? Which features did you use in the final code?**\n",
    "\n",
    "I considered using different n-grams for both word and character. I attempted different token patterns. I attempted different max features and included a library of stop words. Ultimately, the stop words, unigram, and max feature included in the code were the best results.\n",
    "\n",
    "**How did you decide to use the ‘attack’ information from different annotators? Did you average them, or use a number threshold, or did you use some other method to use this information?**\n",
    "\n",
    "I combined the training and development set and chose an average of above 0.5. As long as the majority of annontators agreed that the comment was an attack, than it should suffice to say that it was an attack. This would allow for less unbalanced combination. If I was stricter, say 0.9, this could lead to heavily unbalanced majority bias.\n",
    "\n",
    "**What optimizations did you add in your code, if any?**\n",
    "\n",
    "I used a imblearn pipeline with grid search parameters so that I can insert oversampling and undersampling into the pipeline. This also allowed me to use a formatted list of hyperparameters and implement cross validation.\n",
    "\n",
    "**What are the ML methods you tried out, and what were your best results with each method? Which was the best ML method you saw before tuning hyperparameters?**\n",
    "\n",
    "I tried all the linear models, gradient boosting, and tree models. I even tried using imblearn and added over and under sampling while generating artifical data. The best method, before hyperparameters, in Sklearn was definitely using decision tree classifiers, as they can deal with the imbalanced data better. However, Imblearn models provided more of an advantage in terms of samping and data generation which improved accuracy.\n",
    "\n",
    "**What hyper-parameter tuning did you do, and by how many percentage points did your accuracy go up because of hyper-parameter tuning?**\n",
    "\n",
    "For the randomforest classifier model, I tuned the max depth of the tree and the weight of the data. This greatly increased the recall from the strawman model; however, overall accuracy went down a little bit. For the imblearn model, I chose to sample 3 to 4 neighbors when generating artificial data. I also assigned custom sampling strategies for both over and undersampling and replaced the sampling. This definitely increased the auc roc of the model by 0.02 points.\n",
    "\n",
    "**What did you learn from the different metrics? Did you try cross-validation?**\n",
    "\n",
    "I did attempt cross-validation with GridSearchCV. This made the process easier. The problem with the dataset is that it is heavily imbalanced making the learning algorithm bias toward the majority data. Therefore, AUC ROC scores mean very little if the majority data is always leaning toward False. Therefore, I needed to look at the confusion matrix, precision, and recall values which indicate how many False Positives and False Negatives would be included in the output. This will help determine how to evaluate the performance of the algorithm. In my efforts, I tried my best to keep precision and recall leveled for True while maintaining the integrity of False values. This would hopefully, allow for equally less FN and FP. If the precision is high and the recall low for True, the algorithm would most likely predict more FP. This would be the opposite for FN, where precision is low and recall is high.\n",
    "\n",
    "**What are your best final Result Metrics? What is the increase in accuracy compared to the strawman figure? Which model gave you this performance?**\n",
    "\n",
    "There was a tremendous increase in recall for Attacks and increase in ROC score; however, the accuracy increased by very little. This resulted from the fact that it was already very high to begin with. Overall, this was a trade off for an increase in False Positives, or indicating a comment as an attack when it was not an attack. Where the strawman model could only predict half of the actual attacks as attacks, the improved model using over and under sampling was able to predict 80% of the attacks.\n",
    "\n",
    "**What is the most interesting thing you learned from doing the report?**\n",
    "\n",
    "There are various ways to deal with balanced and unbalanced data. While all methods produced varying results, the context and goal behind how you will use the data impacts how you want to arrive at the results. For example, for our model, the goal could be that we want to flag attacks, but not at the risk of flagging regular comments as attacks. Therefore, we would probably want to go with a model that produces a higher precision score for attacks. I chose to go the route that allows for more accurately labeled attacks at the risk of flagging regular comments as attacks. For unbalanced data, there are models to generate artificial data that could balance the data and help the algortihm perform better. Ultimately, there is no perfect algorithm and there will always be False Negatives or False Positives in the confusion matrix.\n",
    "\n",
    "**What was the hardest thing to do?**\n",
    "\n",
    "Hyperparameter tuning was the most difficult. The parameters had very little or cryptic documentation for some of them. At times, it felt like trial and error. Thus, the more folds or fits that occurs, the longer you have to wait to retrieve the results from the model. All of the models from Sklearn either increased recall but reduced precision or vice versa. I was unable to get the precision and recall to a high level. Imblearn allowed me to achieve this more accurately, although still not to the level that I would expect in this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a classifier for personal attacks\n",
    "In this section we will train a simple bag-of-words classifier for personal attacks using the [Wikipedia Talk Labels: Personal Attacks]() data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction import text \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "Use this section to import the data if you have not downloaded the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download annotated comments and annotations\n",
    "ANNOTATED_COMMENTS_URL = 'https://ndownloader.figshare.com/files/7554634' \n",
    "ANNOTATIONS_URL = 'https://ndownloader.figshare.com/files/7554637' \n",
    "\n",
    "def download_file(url, fname):\n",
    "    urllib.request.urlretrieve(url, fname)\n",
    "\n",
    "                \n",
    "# download_file(ANNOTATED_COMMENTS_URL, 'attack_annotated_comments.tsv')\n",
    "# download_file(ANNOTATIONS_URL, 'attack_annotations.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "comments = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('attack_annotations.tsv',  sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling Data\n",
    "The data is labeled based on majority vote of which comments are attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels a comment as an attack if the majority of annoatators did so\n",
    "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join labels and comments\n",
    "comments['attack'] = labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data\n",
    "Plotting the data to get a better understanding of the balance. Here we see that there is a heavy imbalance of comments marked as attack versus not attacks. The data requires a use of strategies that work better for imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the attacks\n",
    "fig , ax = plt.subplots(figsize=(6,4))\n",
    "sns.countplot(x='attack', data=comments)\n",
    "plt.title(\"Count of Attacks\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "Extranneous information is removed from the comments. Here we removed unimportant tokens for new lines and tabs. The data is split up into two groups, one for training and the other for testing. Since the data is heavily imbalanced, we required additional data points from another table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# remove newline, tab tokens, punctuation, and extra white spaces\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].str.replace(' +', ' ')\n",
    "comments['comment'] = comments['comment'].str.replace('[^\\w\\s]', ' ')\n",
    "\n",
    "# splitting the data\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "# bulking up the data\n",
    "dev_comments = comments.query(\"split=='dev'\")\n",
    "attack_comments = dev_comments.query(\"attack==True\")\n",
    "train_new_comments = train_comments.append(attack_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imblearn Pipelines\n",
    "Here we add a pipeline with hyperparamaters that take advantage of imblearn's under and over sampling strategies. The pipeline also generates artificial data with SMOTE and runs it through a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters for the pipline\n",
    "param_grid = { \n",
    "    \"feature_union__vect_word__analyzer\": ['word'],\n",
    "    \"feature_union__vect_word__token_pattern\": [r'\\w{1,}'],\n",
    "    \"feature_union__vect_word__max_features\": [10000],\n",
    "    \"feature_union__vect_word__lowercase\": [True],\n",
    "    \"feature_union__vect_word__stop_words\": [text.ENGLISH_STOP_WORDS],\n",
    "    \"feature_union__vect_word__ngram_range\": [(1,1),(1,2)],\n",
    "    \"feature_union__vect_word__max_df\": [1.0],\n",
    "    \"feature_union__vect_word__min_df\": [1],\n",
    "    \"smote__k_neighbors\": [3,4],\n",
    "    \"smote__sampling_strategy\": [0.3, 0.2],\n",
    "    \"undersample__replacement\": [True],\n",
    "    \"undersample__sampling_strategy\": [0.7, 0.8]\n",
    "}\n",
    "\n",
    "feature_union = ('feature_union', FeatureUnion([\n",
    "    ('vect_word', TfidfVectorizer()),\n",
    "    ('vect_char', TfidfVectorizer()),\n",
    "]))\n",
    "\n",
    "# pipline for vectorizer, smote, undersampler, and classifier\n",
    "clf = Pipeline(steps=[\n",
    "    feature_union,\n",
    "    ('smote', SMOTE()),\n",
    "    ('undersample',RandomUnderSampler()),\n",
    "    ('classifier', LogisticRegression(max_iter=10000))])\n",
    "\n",
    "# grid search \n",
    "grid_search = GridSearchCV(clf, \n",
    "                           param_grid=param_grid, \n",
    "                           verbose=10, \n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting and Scoring\n",
    "Here we fit the model and print out the scores to see how well the model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done  66 out of  80 | elapsed:  7.0min remaining:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done  75 out of  80 | elapsed:  7.1min remaining:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done  80 out of  80 | elapsed:  7.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.95      0.96     20422\n",
      "        True       0.70      0.80      0.75      2756\n",
      "\n",
      "    accuracy                           0.94     23178\n",
      "   macro avg       0.84      0.88      0.86     23178\n",
      "weighted avg       0.94      0.94      0.94     23178\n",
      "\n",
      "[[19491   931]\n",
      " [  556  2200]]\n",
      "Test ROC AUC: 0.960\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "clf_grid = grid_search.fit(train_new_comments['comment'], train_new_comments['attack'])\n",
    "\n",
    "# true values and predictions\n",
    "truevals = test_comments['attack']\n",
    "prd = clf_grid.predict(test_comments['comment'])\n",
    "\n",
    "auc = roc_auc_score(test_comments['attack'], clf_grid.predict_proba(test_comments['comment'])[:, 1])\n",
    "\n",
    "# print scores\n",
    "print(classification_report(truevals, prd))\n",
    "print(confusion_matrix(truevals, prd))\n",
    "print('Test ROC AUC: %.3f' %auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "Demo of how the classifier works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correctly classify nice comment\n",
    "clf_grid.predict(['Thanks for you contribution, you did a great job!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correctly classify nasty comment\n",
    "clf_grid.predict(['I hate you so much! Go to Hell.'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Pipelines\n",
    "Here we model the data using Sklearn's resampling for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters for the pipline\n",
    "sklearn_param_grid = { \n",
    "    \"vect_word__analyzer\": ['word'],\n",
    "    \"vect_word__token_pattern\": [r'\\w{1,}'],\n",
    "    \"vect_word__max_features\": [10000],\n",
    "    \"vect_word__lowercase\": [True],\n",
    "    \"vect_word__stop_words\": [text.ENGLISH_STOP_WORDS],\n",
    "    \"vect_word__ngram_range\": [(1,2),(1,3)],\n",
    "    \"vect_word__max_df\": [1.0],\n",
    "    \"vect_word__min_df\": [1],\n",
    "    \"classifier__max_depth\": [20],\n",
    "    \"classifier__criterion\": [\"gini\", \"entropy\"],\n",
    "    \"classifier__n_estimators\": [1000],\n",
    "    \"classifier__class_weight\": [{0:1,1:5}]\n",
    "}\n",
    "\n",
    "# pipline for vectorizer, resampling, and classifier\n",
    "sklearn_clf = Pipeline(steps=[\n",
    "    ('vect_word', TfidfVectorizer()),\n",
    "    ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# grid search \n",
    "sklearn_grid_search = GridSearchCV(sklearn_clf, \n",
    "                           param_grid=sklearn_param_grid, \n",
    "                           verbose=10, \n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Fitting and Scoring\n",
    "Here we fit the model and print out the scores to see how well the model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  20 | elapsed:  2.8min remaining: 15.6min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  20 | elapsed:  3.0min remaining:  7.0min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of  20 | elapsed:  3.2min remaining:  3.9min\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  20 | elapsed:  3.2min remaining:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  20 | elapsed:  4.8min remaining:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:  5.1min remaining:   33.9s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  5.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.98      0.96     20422\n",
      "        True       0.81      0.60      0.69      2756\n",
      "\n",
      "    accuracy                           0.94     23178\n",
      "   macro avg       0.88      0.79      0.83     23178\n",
      "weighted avg       0.93      0.94      0.93     23178\n",
      "\n",
      "[[20039   383]\n",
      " [ 1103  1653]]\n",
      "Test ROC AUC: 0.901\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "sklearn_clf_grid = sklearn_grid_search.fit(train_new_comments['comment'], train_new_comments['attack'])\n",
    "\n",
    "# true values and predictions\n",
    "truevals = test_comments['attack']\n",
    "prd = sklearn_clf_grid.predict(test_comments['comment'])\n",
    "\n",
    "auc = roc_auc_score(test_comments['attack'], sklearn_clf_grid.predict_proba(test_comments['comment'])[:, 1])\n",
    "\n",
    "# print scores\n",
    "print(classification_report(truevals, prd))\n",
    "print(confusion_matrix(truevals, prd))\n",
    "print('Test ROC AUC: %.3f' %auc)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
